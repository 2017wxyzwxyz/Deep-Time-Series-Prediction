{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3f8f206dc7ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "\n",
    "#import tensorflow.compat.v1 as tf\n",
    "#tf.disable_v2_behavior()\n",
    "import tensorflow as tf\n",
    "# from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.python.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from os.path import isfile\n",
    "import os\n",
    "# import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "# tensorflow v1版的import\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    #     model = tf.keras.models.Sequential([\n",
    "    #       tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    #       tf.keras.layers.Dense(128, activation='relu'),\n",
    "    #       tf.keras.layers.Dropout(0.2),\n",
    "    #       tf.keras.layers.Dense(10, activation='softmax')\n",
    "    #     ])\n",
    "    model = Sequential() # create model\n",
    "    model.add(Dense(5, input_dim=8, activation='relu')) # hidden layer\n",
    "    model.add(Dense(1, activation='sigmoid'))      \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_to_serving(model, export_version, export_path='prod_models'):\n",
    "    # 只能用v1版本的tensorflow\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    tf.disable_v2_behavior()\n",
    "    print(model.input, model.output)\n",
    "    signature = tf.saved_model.signature_def_utils.predict_signature_def(                                                                        \n",
    "        inputs={'voice': model.input}, outputs={'scores': model.output})\n",
    "    export_path = os.path.join(\n",
    "        tf.compat.as_bytes(export_path),\n",
    "        tf.compat.as_bytes(str(export_version)))\n",
    "    builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n",
    "    legacy_init_op = tf.group(tf.tables_initializer(), name='legacy_init_op')\n",
    "    # tensorflow v1,v2区别在于v2集成了keras，\n",
    "    builder.add_meta_graph_and_variables(\n",
    "        sess=K.get_session(),                                                                                                                    \n",
    "        tags=[tf.saved_model.tag_constants.SERVING],                                                                                             \n",
    "        signature_def_map={                                                                                                                      \n",
    "            'voice_classification': signature,                                                                                                                     \n",
    "        })  #\n",
    "    builder.save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步，定义并训练模型并保存checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/python_code/tensorflow/keras/keras_tensorflow_serving/checkpoint/nn_weights-{epoch:02d}.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[9228.449955174181,\n",
       "  9197.346464178752,\n",
       "  9171.464119640605,\n",
       "  9157.033575031526,\n",
       "  9149.73608829455],\n",
       " [99.29937, 99.04883, 98.851295, 98.74097, 98.68385]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#使用numpy生成假数据。共dataset_size个学习样本\n",
    "dataset_size = 4230\n",
    "x_ =np.random.rand(dataset_size,8)\n",
    "# y = 2*x1 + 7*x2 + 0.3\n",
    "u = map(lambda x1, x2, x3, x4, x5, x6, x7, x8: 2.0*x1 + 7.0*x2 + x3*0.45 + x4*36 + x5*6 + x6*120 + x7*4 + x8*3.5\n",
    "        + 0.3, x_[:, 0], x_[:, 1], x_[:, 2], x_[:, 3], x_[:, 4], x_[:, 5], x_[:, 6], x_[:, 7])\n",
    "y_ = np.array([[i] for i in u])\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 155\n",
    "np.random.seed(seed)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(x_, y_,\n",
    "                                                    test_size=0.25, random_state=87)\n",
    "np.random.seed(seed)\n",
    "my_first_nn = Sequential() # create model\n",
    "my_first_nn.add(Dense(5, input_dim=8, activation='relu')) # hidden layer\n",
    "my_first_nn.add(Dense(1, activation='sigmoid'))           # output layer\n",
    "my_first_nn.compile(loss='mse', optimizer='adam', metrics=['mape'])\n",
    "\n",
    "# optional\n",
    "\n",
    "# specify filepath- this will write a new file for each epoch with the epoch number contained within the filename\n",
    "filename = \"nn_weights-{epoch:02d}.hdf5\"\n",
    "filepath = Path().resolve()/ \"checkpoint\" / filename\n",
    "if os.path.exists(str(filepath.parent)):\n",
    "    #     print(str(filepath.parent))\n",
    "    #os.rmdir(str(filepath.parent))  # 只能删除空文件夹\n",
    "    shutil.rmtree(str(filepath.parent),True) \n",
    "    os.mkdir(str(filepath.parent))\n",
    "else:\n",
    "    os.mkdir(str(filepath.parent))\n",
    "print(filepath)\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(str(filepath), monitor='val_mape', verbose=0,\n",
    "                                             save_weights_only=False, save_best_only=False, mode='min')\n",
    "\n",
    "# verbose=0 suppresses the file writing message\n",
    "# note that the fit method expects a list of callbacks\n",
    "my_first_nn_fitted = my_first_nn.fit(X_train, Y_train, epochs=100, verbose=0, batch_size=32,\n",
    "                                     callbacks=[checkpoint], initial_epoch=0)\n",
    "\n",
    "# [loss, accuracy]\n",
    "my_first_nn.evaluate(X_test, Y_test, verbose=0)\n",
    "# training loss and accuracy over the first five epochs\n",
    "[my_first_nn_fitted.history['loss'][0:5], my_first_nn_fitted.history['mape'][0:5]]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:9225.753902557892,mape:99.14933013916016\n",
      "loss:9194.780341741493,mape:98.90576934814453\n",
      "loss:9174.962120820534,mape:98.75141143798828\n",
      "loss:9164.916641437263,mape:98.67112731933594\n",
      "loss:9159.803304805648,mape:98.62895965576172\n",
      "loss:9157.012649161154,mape:98.60525512695312\n",
      "loss:9155.358987328686,mape:98.59080505371094\n",
      "loss:9154.306452327504,mape:98.58137512207031\n",
      "loss:9153.604095285917,mape:98.574951171875\n",
      "loss:9153.109699905483,mape:98.5703353881836\n",
      "loss:9152.749623405009,mape:98.56690216064453\n",
      "loss:9152.483555352079,mape:98.5643539428711\n",
      "loss:9152.280773718101,mape:98.5623550415039\n",
      "loss:9152.121195282964,mape:98.56076049804688\n",
      "loss:9151.994192314509,mape:98.55946350097656\n",
      "loss:9151.891437263705,mape:98.55842590332031\n",
      "loss:9151.806324949788,mape:98.55755615234375\n",
      "loss:9151.736128751181,mape:98.55680847167969\n",
      "loss:9151.677664224953,mape:98.55619049072266\n",
      "loss:9151.627141422496,mape:98.55567169189453\n",
      "loss:9151.584536345108,mape:98.55519104003906\n",
      "loss:9151.548126624528,mape:98.55480194091797\n",
      "loss:9151.516632945415,mape:98.55445098876953\n",
      "loss:9151.488931060963,mape:98.55415344238281\n",
      "loss:9151.465065276465,mape:98.55386352539062\n",
      "loss:9151.443865193762,mape:98.55363464355469\n",
      "loss:9151.425002215265,mape:98.55340576171875\n",
      "loss:9151.408363731687,mape:98.55320739746094\n",
      "loss:9151.39341254135,mape:98.55303192138672\n",
      "loss:9151.380209564037,mape:98.55287170410156\n",
      "loss:9151.368173292769,mape:98.55274963378906\n",
      "loss:9151.357648939627,mape:98.55261993408203\n",
      "loss:9151.347776243501,mape:98.55249786376953\n",
      "loss:9151.338953952032,mape:98.55238342285156\n",
      "loss:9151.331063917769,mape:98.55229187011719\n",
      "loss:9151.323783081285,mape:98.55220031738281\n",
      "loss:9151.317329276937,mape:98.55213165283203\n",
      "loss:9151.31120407018,mape:98.55204772949219\n",
      "loss:9151.305580990076,mape:98.55197143554688\n",
      "loss:9151.30053387878,mape:98.5519027709961\n",
      "loss:9151.29585597826,mape:98.55184936523438\n",
      "loss:9151.291580517485,mape:98.55181121826172\n",
      "loss:9151.287497046314,mape:98.55175018310547\n",
      "loss:9151.284048617676,mape:98.55171203613281\n",
      "loss:9151.280600189037,mape:98.5516586303711\n",
      "loss:9151.277624350188,mape:98.55160522460938\n",
      "loss:9151.274768504843,mape:98.55158233642578\n",
      "loss:9151.272167414934,mape:98.55154418945312\n",
      "loss:9151.269516481569,mape:98.551513671875\n",
      "loss:9151.267371366966,mape:98.55146026611328\n",
      "loss:9151.265270557657,mape:98.55144500732422\n",
      "loss:9151.263142057538,mape:98.55142211914062\n",
      "loss:9151.261467686672,mape:98.5513916015625\n",
      "loss:9151.2596936289,mape:98.55137634277344\n",
      "loss:9151.258185402883,mape:98.55135345458984\n",
      "loss:9151.256704867676,mape:98.55132293701172\n",
      "loss:9151.255257561437,mape:98.55131530761719\n",
      "loss:9151.25379179466,mape:98.5512924194336\n",
      "loss:9151.252638010988,mape:98.55126953125\n",
      "loss:9151.25155806947,mape:98.55126190185547\n",
      "loss:9151.250402439744,mape:98.5512466430664\n",
      "loss:9151.249412954867,mape:98.55123901367188\n",
      "loss:9151.248554539816,mape:98.55122375488281\n",
      "loss:9151.247594591801,mape:98.55120086669922\n",
      "loss:9151.24678048204,mape:98.55118560791016\n",
      "loss:9151.246189744801,mape:98.5511703491211\n",
      "loss:9151.245418094282,mape:98.5511474609375\n",
      "loss:9151.244605830576,mape:98.55113983154297\n",
      "loss:9151.24392648275,mape:98.5511245727539\n",
      "loss:9151.243439124528,mape:98.5511245727539\n",
      "loss:9151.242962842627,mape:98.55111694335938\n",
      "loss:9151.242386873819,mape:98.55110931396484\n",
      "loss:9151.241737062854,mape:98.55110168457031\n",
      "loss:9151.241294009924,mape:98.55110168457031\n",
      "loss:9151.240937721526,mape:98.55109405517578\n",
      "loss:9151.240598047614,mape:98.55109405517578\n",
      "loss:9151.240228836838,mape:98.55109405517578\n",
      "loss:9151.239697173321,mape:98.55107879638672\n",
      "loss:9151.23935749941,mape:98.55107879638672\n",
      "loss:9151.238971674149,mape:98.55107879638672\n",
      "loss:9151.238646768667,mape:98.55106353759766\n",
      "loss:9151.238513852788,mape:98.55105590820312\n",
      "loss:9151.23826278946,mape:98.55105590820312\n",
      "loss:9151.238070799858,mape:98.55105590820312\n",
      "loss:9151.237849273393,mape:98.55105590820312\n",
      "loss:9151.23764251536,mape:98.55105590820312\n",
      "loss:9151.237343454632,mape:98.55105590820312\n",
      "loss:9151.237018549149,mape:98.55104064941406\n",
      "loss:9151.236767485821,mape:98.551025390625\n",
      "loss:9151.23672318053,mape:98.551025390625\n",
      "loss:9151.236531190927,mape:98.551025390625\n",
      "loss:9151.236324432892,mape:98.551025390625\n",
      "loss:9151.236176748582,mape:98.55101776123047\n",
      "loss:9151.23613244329,mape:98.55101776123047\n",
      "loss:9151.23598475898,mape:98.55101776123047\n",
      "loss:9151.235778000946,mape:98.55101776123047\n",
      "loss:9151.23570415879,mape:98.55101776123047\n",
      "loss:9151.235512169187,mape:98.5510025024414\n",
      "loss:9151.23540879017,mape:98.5510025024414\n"
     ]
    }
   ],
   "source": [
    "# You may also want to visualise how the model performed\n",
    "# on the test set over time. Remember that we saved the model weights\n",
    "# to a file at each epoch. This means we can reproduce each intermediate model\n",
    "# by loading the weights into an appropriately constructed\n",
    "# (i.e. same number of layers/neurons) neural network.\n",
    "temp_test_model = Sequential() # create model\n",
    "temp_test_model.add(Dense(5, input_dim=8, activation='relu')) # hidden layer\n",
    "temp_test_model.add(Dense(1, activation='sigmoid')) # output layer\n",
    "temp_test_model.compile(loss='mse', optimizer='adam', metrics=['mape'])\n",
    "test_over_time = []\n",
    "# print(my_first_nn_fitted.history.keys())\n",
    "for i in range(1,len(my_first_nn_fitted.history['loss'])):\n",
    "    temp_test_model.load_weights(str(Path().resolve()/ \"checkpoint\" / \"nn_weights-%02d.hdf5\") % i)\n",
    "    scores = temp_test_model.evaluate(X_test, Y_test, verbose=0)\n",
    "    # 0 is loss; 1 is mape\n",
    "    print(\"loss:{},mape:{}\".format(scores[0],scores[1]))\n",
    "    test_over_time.append(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步，重新用tensorflow定义模型，并将keras训练的权重填充新模型的权重，将新模型保存成tensorflow serving可以识别的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1030 15:53:32.362629 140154643892032 deprecation.py:323] From /root/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "W1030 15:53:32.366569 140154643892032 deprecation.py:323] From /root/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 5)                 45        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 51\n",
      "Trainable params: 51\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "No checkpoint file detected.  Starting from scratch.\n",
      "Tensor(\"dense_4_input:0\", shape=(?, 8), dtype=float32) Tensor(\"dense_5/Identity:0\", shape=(?, 1), dtype=float32)\n",
      "save done!\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "checkpoint_filepath = './checkpoint/nn_weights-999.hdf5'\n",
    "if (isfile(checkpoint_filepath)):\n",
    "    print('Checkpoint file detected. Loading weights.')\n",
    "    model.load_weights(checkpoint_filepath) # 加载模型\n",
    "else:\n",
    "    print('No checkpoint file detected.  Starting from scratch.')\n",
    "\n",
    "export_path = \"test_model\"\n",
    "save_model_to_serving(model, \"1\", export_path)\n",
    "print(\"save done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl]",
   "language": "python",
   "name": "conda-env-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
